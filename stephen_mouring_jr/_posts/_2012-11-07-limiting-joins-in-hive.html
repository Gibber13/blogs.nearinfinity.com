---
title: Limiting Joins in Apache Hive
tags:  hive hadoop mapreduce bigdata
---

Working with large datasets in Hadoop / Hive works very well up until you have an "imbalanced" join. If you are trying to join two large tables together where a large amount of data in one table joins to a one or a few keys in the other table, individual reducers will get overloaded and slow your job down to the point where it may never finish.<br/><br/>

There are several ways to tackle this problem. Sometimes you can solve this by improving the quality of your data or filtering it with a where clause. Sometimes you can modify the join condition to include additional constraints that force the data to be better partitioned. Or you could try using Hive's support for "skew" joins where Hive handles dividing up the data landing on the same key, running it on separate reducers then merging the results. Sometimes none of these options are feasible or desirable. So what are your alternatives?
<br/><br/>

One common case I have seen is where you have a null or blank key. Whenever you have a query in which all the reducers complete very quick except for one, you are often in this situation. You can tell especially when it is the zero reducer (the reducer is is 000000), because that is the reducer that null or blank keys are hashed to by Map Reduce by default.
<br/><br/>

Sometimes you can just ignore these records in the join:
<br/><br/>

{% highlight sql %}
select
   *
from
   foo
join
   bar on foo.bar_id = bar.id and foo.bar_id is not null
{% endhighlight %}<br /><br />

This does not always work because sometimes you need to keep the records with the null ids in the query, especially if you have nested joins. So you can do something like this:
<br/><br/>

{% highlight sql %}
select
   f.*,
   b.barColumn1,
   b.barColumn2
from
   foo f
join
   bar b on f.bar_id = b.id and f.bar_id is not null

union all

select
   f.*,
   '' as barColumn1,
   '' as barColumn2
from
   foo f
where
   f.bar_id is null
{% endhighlight %}<br /><br />

(Note: It would be tempting to use a left outer join here, but the problem is all the records in "foo" that have a null "bar_id" would still land on the same reducer, even though they do not join to anything.)</i>
<br/><br/>

This is all well and good for blank and null keys... But what about cases where you legitimately have extreme distributions of data?
<br/><br/>

I recently encountered an example in a dataset where less than 1% of the records in one table (Table A) joined to 50% of the records in another (Table B). The amount of data returned by the top 1% in Table A was so great our webservice could not handle returning it to the client. So we decided to prune the top 1% of Table A and only keep the remaining 99% of the records.
<br/><br/>

My first attempt went something like this:
<br/><br/>

{% highlight sql %}
select
   f.*,
   barJoin.*
from
   foo f
join (
   select
      b.*,
      bc.barCount
   from
      bar b
   join (
      select id, count(*) as barCount from bar group by id
   ) bc on b.id = bc.id

) barJoin on f.bar_id = barJoin.id and barJoin.barCount < 1000
{% endhighlight %}<br /><br />

A little complex at first, but the basic approach was to get a count of how many "bar" records there are per id value (that is the inner join). Then when you join foo and bar, you can filter based on the count.
<br/><br/>

<b>PROBLEM!</b> Hive does not allow <i>non equality</i> conditions in joins. So the "barCount &lt; 1000" is <b>silently ignored by hive</b>. Bummer.<br /><br />

Thankfully there are two alternatives. The next simplest thing to do is this:

{% highlight sql %}
select
   f.*,
   barJoin.*
from
   foo f
join (
   select
      b.*,
      bc.barCount
   from
      bar b
   join (
      select id, count(*) as barCount from bar group by id having barCount < 1000
   ) bc on b.id = bc.id
) barJoin on f.bar_id = barJoin.id
{% endhighlight %}<br /><br />

Note that I removed the conditional in the join clause and added a having clause to the inner join. This whittles down the invalid "bar" ids before they join, which removes the bad keys from "foo" as well.
<br /><br />

A good solution, but what if you want to preserve the bad keys in "foo" but just omitted the joined data?
<br /><br />

{% highlight sql %}
select
   f.*,
   barJoin.*
from
   foo f
join (
   select
      b.*,
      bc.barCount,
      if (barCount < 1000, 'N', 'Y') as prune
   from
      bar b
   join (
      select id, count(*) as barCount from bar group by id
   ) bc on b.id = bc.id
) barJoin on f.bar_id = barJoin.id and barJoin.prune = 'N'
{% endhighlight %}<br /><br />

I removed the having clause, but added an "if" statement to the inner select to create a flag indicating whether or not the joined data should be preserved. The outer join condition can now use the state of the "prune" flag (which is an equality operation!) to select which records should join.
<br /><br />

Please comment with any questions or additional thoughts!
