--- 
permalink: /blogs/seth_schroeder/unicode_code_points_vs_code.html
layout: blogs
title: Unicode code points vs code units
date: 2007-08-23 22:53:32 -04:00
tags: General
---
{% raw %}
What's the difference between a Unicode code point and a code unit? Roughly, code points are for people and code units are for computers. 

It takes five code points to say Hello, and they are:<code> 72, 101, 108, 108, 111</code>
<p>
Each code <b>unit</b> format has its own structure. Note that UTF-16LE needs an extra code unit.
</p><p>UTF-8:<code class="prettyprint"> 0x48, 0x65, 0x6c, 0x6c, 0x6f</code>
<br />UTF-16: <code class="prettyprint"> 0x0048, 0x0065, 0x006c, 0x006c, 0x006f</code>
<br />UTF-16LE: <code class="prettyprint"> 0xfffe, 0x4800, 0x6500, 0x6c00, 0x6c00, 0x6f00</code>
<br />UTF-32: <code class="prettyprint"> 0x00000048, 0x00000065, 0x0000006c, 0x0000006c, 0x0000006f</code>
</p><p>
So... what? Well, someone had a thinking cap on when they defined UTF-8. It's optimal for storing American English and safe for legacy software libraries. My favorite part is how the first bits in the first byte count the number of bytes remaining in the code unit. 
</p><p>
UTF-16 is kinda strange when you really look at it. Storing it requires 100% more space than UTF-8 requires for American English. It's big/network endian by default... meaning most computers have to juggle bytes <b>per character</b> before using them. And it's totally incompatible with old ASCII libraries. Finally, it's not possible to rely on 1 utf-16 code unit per code point due to surrogate characters.
</p><p>
The Unicode 5.0 spec is gigantic. It's always stocked on the top shelf at Borders with the other oversized books. Read it and you may believe that ZWNBSP means something. :)</p> 
{% endraw %}
