--- 
permalink: /blogs/lucene_is_a_memory_hog.html
layout: blogs
title: Lucene is a memory hog!
date: 2008-09-03 22:29:41 -04:00
tags: Java Lucene
---
<p><span style="font-family: Arial;">Let me start by saying that, I like Lucene, I have used it to solve many technical problems on my current project. But one aspect of Lucene that I have had issues with is with its memory footprint.</span></p>

<p><span style="font-family: Arial;">Currently we index 38 fields across 1.5 billion documents, and we have implemented a fair similarity object (see <a href="http://lucene.apache.org/java/docs/scoring.html">Lucene Scoring Documentation</a>) for normalized scoring. We have no use for index time field boosting or for any type of Norms (see <a href="http://lucene.apache.org/java/docs/scoring.html">Lucene Scoring Documentation</a>). However Lucene reads all of the Norms into memory for fast scoring. </span></p>

<p><span style="font-family: Arial;">So let's do the math:</span></p>

<p><span style="font-family: Arial;">1 byte per Norm value * 38 fields * 1,500,000,000 = <b><i>57,000,000,000</i></b></span></p>

<p><span style="font-family: Arial;">That's +/- 57 Gigs of heap space!</span></p>

<p><span style="font-family: Arial;">That's quiet a bit of memory usage for something that we don't even use. I have since patched Lucene, so that the indexed Norms have a much better memory footprint, something around 1.5 Gigs. Not great, but livable.</span></p>

<p><span style="font-family: Arial;">I havenÃ•t posted my patch, because itÃ•s all or nothing, I havenÃ•t implemented a way to turn it on and off. But if there is anyone else out there, with an application that is running out of memory as users use more and more indexed fields in Lucene, take a look at the <i>SegmentReader</i> class. There's a byte array in there that you should take a look at. Happy hunting!</span></p> 
